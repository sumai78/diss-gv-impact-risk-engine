# -*- coding: utf-8 -*-
"""02_train_models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M18_gEVjfFAsR-47wP3SKER4ZLGIWnKa

# NOTE (reproducibility + feature choice):
# This notebook was executed in Google Colab, so intermediate files are written to /content/ (runtime storage).
# The cleaned dataset used for modelling contains 12 columns by design: only modelling features + target are kept.
# Identifier fields (e.g., loan_id) are intentionally excluded from feature_cols to avoid leakage and spurious learning.
# If an ID is needed later (e.g., for a Streamlit “select loan” demo), it is treated as metadata only, not a model input.
"""

!cp /content/drive/MyDrive/01_clean_features.py /content/

!python 01_clean_features.py

# (If previously saw 13 columns, that included an ID/metadata column; it was removed here to keep modelling inputs clean.)

# 02_train_logistic.py
# GreenVi$or: Baseline Logistic Regression viability model

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, classification_report
import matplotlib.pyplot as plt

# Step A: load cleaned dataset created by 01_clean_features.py
DATA_PATH = "/content/kiva_processed.csv"
df = pd.read_csv(DATA_PATH)

# Step B: define target label and modelling features
target = "y_funded"

feature_cols = [
    "loan_amount",
    "lender_term",
    "sector_name",
    "activity_name",
    "country_name",
    "repayment_interval",
    "borrower_pictured",
    "borrower_genders",
    "partner_id",
]

X = df[feature_cols]
y = df[target]

num_cols = ["loan_amount", "lender_term", "partner_id"]
cat_cols = [c for c in feature_cols if c not in num_cols]

# Step C: build preprocessing pipelines for numerical and categorical features
preprocess = ColumnTransformer(
    transformers=[
        ("num", Pipeline([("imp", SimpleImputer(strategy="median"))]), num_cols),
        ("cat", Pipeline([
            ("imp", SimpleImputer(strategy="most_frequent")),
            ("oh", OneHotEncoder(handle_unknown="ignore"))
        ]), cat_cols),
    ]
)

# Step D: define baseline Logistic Regression viability model
model = Pipeline([
    ("prep", preprocess),
    ("clf", LogisticRegression(
        max_iter=3000,
        class_weight="balanced",
        solver="lbfgs"
    ))
])

# Step E: split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Step F: train the baseline viability model
model.fit(X_train, y_train)

# Step G: generate predicted probabilities and class predictions
proba = model.predict_proba(X_test)[:, 1]
pred = (proba >= 0.5).astype(int)

# Step H: compute evaluation metrics
auc = roc_auc_score(y_test, proba)
print("ROC-AUC:", round(auc, 4))
print(classification_report(y_test, pred, digits=3))

# Step I: save ROC curve figure for reporting
from sklearn.metrics import roc_curve

fpr, tpr, _ = roc_curve(y_test, proba)

plt.figure()
plt.plot(fpr, tpr)
plt.plot([0,1],[0,1],'--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Logistic Regression ROC Curve")
plt.savefig("/content/roc_lr.png", dpi=200, bbox_inches="tight")
plt.close()

print("Saved ROC curve to /content/roc_lr.png")

# Random Forest: Non-linear comparator



from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, classification_report, roc_curve
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Step J: create a balanced subsample for Random Forest training
# (expired loans are kept in full and funded loans are downsampled to match)
df_exp = df[df["y_funded"] == 0]
df_fun = df[df["y_funded"] == 1].sample(n=len(df_exp), random_state=42)

df_rf = pd.concat([df_exp, df_fun]).sample(frac=1, random_state=42)  # shuffle

X_rf = df_rf[feature_cols]
y_rf = df_rf["y_funded"]

# Step K: split the sampled dataset into training and test sets
X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(
    X_rf, y_rf, test_size=0.2, stratify=y_rf, random_state=42
)

# Step L: define Random Forest comparator model using the same preprocessing pipeline
rf_model = Pipeline([
    ("prep", preprocess),
    ("clf", RandomForestClassifier(
        n_estimators=200,
        random_state=42,
        n_jobs=-1,
        class_weight="balanced_subsample"
    ))
])
# Step M: train the Random Forest comparator model
rf_model.fit(X_train_rf, y_train_rf)

# Step N: generate predicted probabilities and class predictions
rf_proba = rf_model.predict_proba(X_test_rf)[:, 1]
rf_pred = (rf_proba >= 0.5).astype(int)

# Step O: compute evaluation metrics for the Random Forest model
rf_auc = roc_auc_score(y_test_rf, rf_proba)
print("Random Forest ROC-AUC (sampled):", round(rf_auc, 4))
print(classification_report(y_test_rf, rf_pred, digits=3))

# Step P: save ROC curve figure for Random Forest comparator
fpr_rf, tpr_rf, _ = roc_curve(y_test_rf, rf_proba)

plt.figure()
plt.plot(fpr_rf, tpr_rf)
plt.plot([0,1],[0,1],'--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Random Forest ROC Curve (sampled)")
plt.savefig("/content/roc_rf.png", dpi=200, bbox_inches="tight")
plt.close()

print("Saved RF ROC curve to /content/roc_rf.png")

from sklearn.calibration import calibration_curve

# Step Q: compute calibration curve for Logistic Regression predictions
lr_prob_true, lr_prob_pred = calibration_curve(y_test, proba, n_bins=10)

plt.figure()
plt.plot(lr_prob_pred, lr_prob_true, marker='o')
plt.plot([0,1],[0,1],'--')
plt.xlabel("Predicted Probability")
plt.ylabel("Observed Frequency")
plt.title("Logistic Regression Calibration Curve")
plt.savefig("/content/calibration_lr.png", dpi=200, bbox_inches="tight")
plt.close()

# Step R: compute calibration curve for Random Forest predictions
rf_prob_true, rf_prob_pred = calibration_curve(y_test_rf, rf_proba, n_bins=10)

plt.figure()
plt.plot(rf_prob_pred, rf_prob_true, marker='o')
plt.plot([0,1],[0,1],'--')
plt.xlabel("Predicted Probability")
plt.ylabel("Observed Frequency")
plt.title("Random Forest Calibration Curve")
plt.savefig("/content/calibration_rf.png", dpi=200, bbox_inches="tight")
plt.close()

print("Saved calibration curves.")

# Step S: combine Logistic Regression and Random Forest calibration plots into a single comparison figure
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

img_lr = mpimg.imread("/content/calibration_lr.png")
img_rf = mpimg.imread("/content/calibration_rf.png")

plt.figure(figsize=(10,4))

plt.subplot(1,2,1)
plt.imshow(img_lr)
plt.axis("off")
plt.title("(a) Logistic Regression")

plt.subplot(1,2,2)
plt.imshow(img_rf)
plt.axis("off")
plt.title("(b) Random Forest (sampled)")

plt.tight_layout()
plt.savefig("/content/figure4_calibration_compare.png", dpi=200, bbox_inches="tight")
plt.close()

print("Saved Figure 4 to /content/figure4_calibration_compare.png")

# Step T: compute confusion matrices and precision–recall metrics for both models
from sklearn.metrics import confusion_matrix, average_precision_score

print("---- Logistic Regression (full test split) ----")
print("Confusion matrix:\n", confusion_matrix(y_test, pred))
print("PR-AUC for expired (treat expired as positive):", round(average_precision_score(1 - y_test, 1 - proba), 4))

print("\n---- Random Forest (sampled test split) ----")
print("Confusion matrix:\n", confusion_matrix(y_test_rf, rf_pred))
print("PR-AUC for expired (treat expired as positive):", round(average_precision_score(1 - y_test_rf, 1 - rf_proba), 4))