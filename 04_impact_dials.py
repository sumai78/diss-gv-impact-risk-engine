# -*- coding: utf-8 -*-
"""04_impact_dials.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q6DrykOkLTWBG6d6bRuNw0V-pJzb1pqG
"""

# 04_impact_dials.py
# GreenVi$or: proxy-based Impact Dials (Environmental / Social / Circularity)
# Transparent + auditable: rule-based proxies (not ML)

import os
import pandas as pd


# Step A — Load cleaned dataset + quick reader-friendly snapshot


# A1) input/output paths
DATA_PATH = "data/processed/kiva_processed.csv"
OUT_PATH  = "results/impact_dials.csv"

# A2) Create output folder so saving never fails
os.makedirs("results", exist_ok=True)

# A3) Load cleaned dataset produced by 01_clean_features.py
loans = pd.read_csv(DATA_PATH)

# A4) Print a small snapshot of the dataset
# (used for screenshots in the dissertation appendix)
print("\n[Snapshot] Dataset loaded:", DATA_PATH)
print("Rows:", loans.shape[0], "Cols:", loans.shape[1])
print("Has loan_id:", "loan_id" in loans.columns)
print("Columns:", list(loans.columns))

# Show a few important columns for verification
key_cols = ["loan_id", "status", "y_funded", "sector_name", "activity_name", "country_name", "loan_amount"]
key_cols = [c for c in key_cols if c in loans.columns]
print("\n[Snapshot] Key columns preview (first 5 rows):")
print(loans[key_cols].head(5))


# Step B — Proxy rules (simple, explainable)


# B1) Environmental proxy: sector/activity weights (transparent heuristic)
# NOTE: This is NOT a true emissions model. It is a policy style proxy for relative impact relevance.
SECTOR_ENV_WEIGHT = {
    "Agriculture": 8,
    "Manufacturing": 9,
    "Retail": 5,
    "Clothing": 6,
    "Services": 4,
    "Education": 2,
    "Health": 3,
    "Food": 6,
    "Construction": 7,
    "Transportation": 7,
}

# Optional activity level boost (kept small to avoid overclaiming)
ACTIVITY_ENV_BOOST = {
    "Recycling": 2,
    "Solar": 2,
    "Organic": 1,
    "Farming": 1,
    "Composting": 2,
}

# B2) Social proxy: keyword weights (simple text scan)
SOCIAL_KW = {
    "women": 3, "woman": 3, "girls": 3,
    "youth": 2, "children": 2,
    "refugee": 3, "disabled": 3,
    "education": 2, "school": 2,
    "health": 2, "clinic": 2
}

# B3) Circularity proxy: keyword weights (simple text scan)
CIRC_KW = {
    "recycle": 3, "recycling": 3,
    "reuse": 3, "repair": 2,
    "refurb": 2, "second-hand": 2,
    "organic": 2, "eco": 2,
    "sustainable": 2, "compost": 2
}

def keyword_score(text: str, kw: dict) -> int:
    """
    Simple keyword proxy:
    - lowercase the text
    - add weights if a keyword appears
    Transparent + auditable, but limited (no context / negation / semantics).
    """
    if not isinstance(text, str):
        return 0
    t = text.lower()
    return sum(weight for word, weight in kw.items() if word in t)

def minmax_0_10(s: pd.Series) -> pd.Series:
    """
    Convert a raw proxy into a 0–10 dial using min–max scaling.
    If all values are identical, return a neutral score (5.0).
    """
    if s.max() == s.min():
        return pd.Series([5.0] * len(s), index=s.index)
    return 10 * (s - s.min()) / (s.max() - s.min())

def band_label(x: float) -> str:
    """
    Human-readable banding for reporting:
    - 0–4   = Low
    - 4–7   = Medium
    - 7–10  = High
    """
    if x < 4:
        return "Low"
    if x < 7:
        return "Medium"
    return "High"


# Step C  Build raw scores (env/social/circular) + normalise


# C1) Environmental raw: sector baseline + small activity boost (if present)
loans["env_raw"] = 5  # safe default

if "sector_name" in loans.columns:
    loans["env_raw"] = loans["sector_name"].map(SECTOR_ENV_WEIGHT).fillna(5).astype(int)

# Small optional boost from activity_name (only if column exists)
if "activity_name" in loans.columns:
    boost = loans["activity_name"].map(ACTIVITY_ENV_BOOST).fillna(0).astype(int)
    loans["env_raw"] = (loans["env_raw"] + boost).clip(lower=0, upper=10)

# C2) Social raw: keyword scoring (from text if available)
if "text" in loans.columns:
    loans["social_raw"] = loans["text"].apply(lambda x: keyword_score(x, SOCIAL_KW))
else:
    loans["social_raw"] = 0

# C3) Circular raw: keyword scoring (from text if available)
if "text" in loans.columns:
    loans["circular_raw"] = loans["text"].apply(lambda x: keyword_score(x, CIRC_KW))
else:
    loans["circular_raw"] = 0

# C4) Normalise to 0–10 dials
loans["env_0_10"]      = minmax_0_10(loans["env_raw"])
loans["social_0_10"]   = minmax_0_10(loans["social_raw"])
loans["circular_0_10"] = minmax_0_10(loans["circular_raw"])

# C5) Add band labels (for easy reporting narrative)
loans["env_band"]      = loans["env_0_10"].apply(band_label)
loans["social_band"]   = loans["social_0_10"].apply(band_label)
loans["circular_band"] = loans["circular_0_10"].apply(band_label)


# Step D — Export an auditable dial table (joinable by loan_id)


out_cols = [
    "loan_id",
    "env_0_10", "env_band",
    "social_0_10", "social_band",
    "circular_0_10", "circular_band"
]
out_cols = [c for c in out_cols if c in loans.columns]

impact_dials = loans[out_cols].copy()
impact_dials.to_csv(OUT_PATH, index=False)

print("\nSaved impact dials to:", OUT_PATH)
print("Rows:", impact_dials.shape[0], "Cols:", impact_dials.shape[1])

print("\n[Output preview] First 5 rows:")
print(impact_dials.head(5))

print("\n[Band distribution] Environmental:", impact_dials["env_band"].value_counts().to_dict())
print("[Band distribution] Social:", impact_dials["social_band"].value_counts().to_dict())
print("[Band distribution] Circularity:", impact_dials["circular_band"].value_counts().to_dict())