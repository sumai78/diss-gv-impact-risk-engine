# -*- coding: utf-8 -*-
"""03_shap_explain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y4z6BciiCiey1nNTzyfiNZ8U0uN3dORc
"""

# 03_shap_explain.ipynb
# Simple global explainability for baseline LR


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression

# Step A: load the cleaned modelling dataset
DATA_PATH = "data/processed/kiva_processed.csv"
df = pd.read_csv(DATA_PATH)

# Step B: define the same modelling features and target label used in model training
feature_cols = [
    "loan_amount",
    "lender_term",
    "sector_name",
    "activity_name",
    "country_name",
    "repayment_interval",
    "borrower_pictured",
    "borrower_genders",
    "partner_id",
]

X = df[feature_cols]
y = df["y_funded"]

num_cols = ["loan_amount", "lender_term", "partner_id"]
cat_cols = [c for c in feature_cols if c not in num_cols]

# Step C: rebuild the preprocessing pipeline (identical to the training script))
preprocess = ColumnTransformer(
    transformers=[
        ("num", Pipeline([("imp", SimpleImputer(strategy="median"))]), num_cols),
        ("cat", Pipeline([
            ("imp", SimpleImputer(strategy="most_frequent")),
            ("oh", OneHotEncoder(handle_unknown="ignore"))
        ]), cat_cols),
    ]
)

# Step D: define a Logistic Regression model for explainability analysis
lr = Pipeline([
    ("prep", preprocess),
    ("clf", LogisticRegression(max_iter=3000, class_weight="balanced"))
])

# Step E: draw a stratified sample for explainability (full dataset not required)
df_s = df.sample(n=100000, random_state=42)
X_s = df_s[feature_cols]
y_s = df_s["y_funded"]

X_train, X_test, y_train, y_test = train_test_split(
    X_s, y_s, test_size=0.2, stratify=y_s, random_state=42
)
# Step F: train the explainability model on the sampled dataset
lr.fit(X_train, y_train)

# Step G: extract one-hot encoded feature names for interpretation
ohe = lr.named_steps["prep"].named_transformers_["cat"].named_steps["oh"]
cat_feature_names = ohe.get_feature_names_out(cat_cols)

all_feature_names = np.concatenate([num_cols, cat_feature_names])

# Step H: compute global feature importance using absolute Logistic Regression coefficients
coefs = lr.named_steps["clf"].coef_.ravel()
abs_coefs = np.abs(coefs)

top_k = 15
top_idx = np.argsort(abs_coefs)[-top_k:][::-1]

top_features = all_feature_names[top_idx]
top_values = abs_coefs[top_idx]

# Step I: save global explainability figure
plt.figure(figsize=(8,5))
plt.barh(top_features[::-1], top_values[::-1])
plt.xlabel("Absolute coefficient magnitude")
plt.title("Global feature importance â€“ Logistic Regression")
plt.tight_layout()
plt.savefig("/content/fig_shap_like_lr.png", dpi=200)
plt.close()

print("Saved explainability figure to /content/fig_shap_like_lr.png")
print("Top drivers:", list(top_features[:10]))

# local explanation (single loan)


# Step J: select one example loan for minimal local explanation
one_loan = X_test.iloc[[0]]
proba = lr.predict_proba(one_loan)[0][1]

print("Predicted funding probability:", round(proba, 3))

# Step K: compute local feature contributions for the selected loan
coefs = lr.named_steps["clf"].coef_.ravel()
encoded = lr.named_steps["prep"].transform(one_loan).toarray()[0]
contrib = encoded * coefs

top_local = np.argsort(np.abs(contrib))[-5:][::-1]

print("Top drivers for this loan:")
for i in top_local:
    print(all_feature_names[i], round(contrib[i], 3))

# 03b dowhy causal check

# Step L: perform a minimal causal plausibility check using DoWhy

# Optional dependency (runs only if DoWhy is available)
!pip install dowhy

from dowhy import CausalModel


df_c = df_s.copy()


# Step M: define causal roles (treatment, outcome and confounders)
causal_model = CausalModel(
    data=df_c,
    treatment="loan_amount",
    outcome="y_funded",
    common_causes=["lender_term", "sector_name", "country_name"]
)

# Step N: identify the causal estimand
identified_estimand = causal_model.identify_effect()

# Step O: estimate causal effect using linear backdoor adjustment
estimate = causal_model.estimate_effect(
    identified_estimand,
    method_name="backdoor.linear_regression"
)

print("Estimated causal effect of loan_amount on funding probability:")
print(estimate.value)