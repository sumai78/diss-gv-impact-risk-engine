# -*- coding: utf-8 -*-
"""03_shap_explain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y4z6BciiCiey1nNTzyfiNZ8U0uN3dORc
"""

# ============================================
# 03_shap_explain.ipynb
# Simple global explainability for baseline LR
# ============================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression

# Step 1 Load cleaned modelling dataset
DATA_PATH = "/content/kiva_processed.csv"
df = pd.read_csv(DATA_PATH)

# Step 2 Same features used in training
feature_cols = [
    "loan_amount",
    "lender_term",
    "sector_name",
    "activity_name",
    "country_name",
    "repayment_interval",
    "borrower_pictured",
    "borrower_genders",
    "partner_id",
]

X = df[feature_cols]
y = df["y_funded"]

num_cols = ["loan_amount", "lender_term", "partner_id"]
cat_cols = [c for c in feature_cols if c not in num_cols]

# Step 3 Preprocessing (same as model training)
preprocess = ColumnTransformer(
    transformers=[
        ("num", Pipeline([("imp", SimpleImputer(strategy="median"))]), num_cols),
        ("cat", Pipeline([
            ("imp", SimpleImputer(strategy="most_frequent")),
            ("oh", OneHotEncoder(handle_unknown="ignore"))
        ]), cat_cols),
    ]
)

# Step 4 Train a small Logistic Regression model for explainability
lr = Pipeline([
    ("prep", preprocess),
    ("clf", LogisticRegression(max_iter=3000, class_weight="balanced"))
])

# Use a small sample for speed (global explainability does not need full dataset)
df_s = df.sample(n=100000, random_state=42)
X_s = df_s[feature_cols]
y_s = df_s["y_funded"]

X_train, X_test, y_train, y_test = train_test_split(
    X_s, y_s, test_size=0.2, stratify=y_s, random_state=42
)

lr.fit(X_train, y_train)

# Step 5 Get one-hot encoded feature names
ohe = lr.named_steps["prep"].named_transformers_["cat"].named_steps["oh"]
cat_feature_names = ohe.get_feature_names_out(cat_cols)

all_feature_names = np.concatenate([num_cols, cat_feature_names])

# Step 6 Global feature importance (absolute coefficients)
coefs = lr.named_steps["clf"].coef_.ravel()
abs_coefs = np.abs(coefs)

top_k = 15
top_idx = np.argsort(abs_coefs)[-top_k:][::-1]

top_features = all_feature_names[top_idx]
top_values = abs_coefs[top_idx]

# Step 7 Plot explainability figure
plt.figure(figsize=(8,5))
plt.barh(top_features[::-1], top_values[::-1])
plt.xlabel("Absolute coefficient magnitude")
plt.title("Global feature importance â€“ Logistic Regression")
plt.tight_layout()
plt.savefig("/content/fig_shap_like_lr.png", dpi=200)
plt.close()

print("Saved explainability figure to /content/fig_shap_like_lr.png")
print("Top drivers:", list(top_features[:10]))

# ============================================
# Minimal local explanation (single loan)
# ============================================

# Pick one funded loan
one_loan = X_test.iloc[[0]]
proba = lr.predict_proba(one_loan)[0][1]

print("Predicted funding probability:", round(proba, 3))

# Get its encoded feature contributions
coefs = lr.named_steps["clf"].coef_.ravel()
encoded = lr.named_steps["prep"].transform(one_loan).toarray()[0]
contrib = encoded * coefs

top_local = np.argsort(np.abs(contrib))[-5:][::-1]

print("Top drivers for this loan:")
for i in top_local:
    print(all_feature_names[i], round(contrib[i], 3))

# ============================================
# 03b dowhy causal check
# ============================================

!pip install dowhy

from dowhy import CausalModel

# Use the same sampled dataframe used for SHAP
df_c = df_s.copy()

# Define causal roles
# Treatment  = decision variable
# Outcome    = model target
# Confounders = structural context variables

causal_model = CausalModel(
    data=df_c,
    treatment="loan_amount",
    outcome="y_funded",
    common_causes=["lender_term", "sector_name", "country_name"]
)

# Identify causal estimand
identified_estimand = causal_model.identify_effect()

# Estimate causal effect using minimal linear backdoor adjustment
estimate = causal_model.estimate_effect(
    identified_estimand,
    method_name="backdoor.linear_regression"
)

print("Estimated causal effect of loan_amount on funding probability:")
print(estimate.value)